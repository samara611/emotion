{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a944873-9faf-4a73-9e31-d2392aa31af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import required packages\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ed8f98-2406-4696-8882-85af99714ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 78, 78, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 39, 39, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 37, 37, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 18, 18, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                524352    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 617674 (2.36 MB)\n",
      "Trainable params: 617674 (2.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(80, 80, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output to feed into dense layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='sigmoid'))  # Using sigmoid for binary classification\n",
    "\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25761d05-63b8-47e9-ba80-cc49ab40f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all test images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Preprocess all train images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'data/test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b72cb50-9355-4eda-a1a7-14d0b5b5187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ...\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fa2e59-b04d-4c02-9b99-cd01af43dec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8929/141623965.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 436s 964ms/step - loss: 1.7120 - accuracy: 0.3064 - val_loss: 1.5100 - val_accuracy: 0.4037\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 375s 834ms/step - loss: 1.4269 - accuracy: 0.4501 - val_loss: 1.2940 - val_accuracy: 0.5014\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 354s 791ms/step - loss: 1.2722 - accuracy: 0.5153 - val_loss: 1.1977 - val_accuracy: 0.5407\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 356s 794ms/step - loss: 1.1991 - accuracy: 0.5410 - val_loss: 1.1605 - val_accuracy: 0.5617\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 355s 793ms/step - loss: 1.1339 - accuracy: 0.5702 - val_loss: 1.1230 - val_accuracy: 0.5718\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 349s 779ms/step - loss: 1.0842 - accuracy: 0.5892 - val_loss: 1.0895 - val_accuracy: 0.5850\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 348s 777ms/step - loss: 1.0386 - accuracy: 0.6098 - val_loss: 1.0777 - val_accuracy: 0.5904\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 346s 772ms/step - loss: 0.9901 - accuracy: 0.6271 - val_loss: 1.0679 - val_accuracy: 0.6002\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 345s 770ms/step - loss: 0.9530 - accuracy: 0.6450 - val_loss: 1.0702 - val_accuracy: 0.5997\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 346s 773ms/step - loss: 0.9157 - accuracy: 0.6573 - val_loss: 1.0524 - val_accuracy: 0.6059\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 349s 779ms/step - loss: 0.8763 - accuracy: 0.6715 - val_loss: 1.0483 - val_accuracy: 0.6101\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 351s 784ms/step - loss: 0.8325 - accuracy: 0.6882 - val_loss: 1.0425 - val_accuracy: 0.6124\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 347s 774ms/step - loss: 0.8035 - accuracy: 0.7035 - val_loss: 1.0551 - val_accuracy: 0.6172\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 350s 781ms/step - loss: 0.7645 - accuracy: 0.7145 - val_loss: 1.0632 - val_accuracy: 0.6117\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 351s 783ms/step - loss: 0.7297 - accuracy: 0.7272 - val_loss: 1.0659 - val_accuracy: 0.6130\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 468s 1s/step - loss: 0.6959 - accuracy: 0.7397 - val_loss: 1.0841 - val_accuracy: 0.6159\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 415s 927ms/step - loss: 0.6795 - accuracy: 0.7459 - val_loss: 1.1140 - val_accuracy: 0.6176\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 423s 944ms/step - loss: 0.6598 - accuracy: 0.7537 - val_loss: 1.0829 - val_accuracy: 0.6198\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 419s 936ms/step - loss: 0.6282 - accuracy: 0.7717 - val_loss: 1.0858 - val_accuracy: 0.6193\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 382s 852ms/step - loss: 0.5984 - accuracy: 0.7802 - val_loss: 1.1019 - val_accuracy: 0.6247\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 360s 803ms/step - loss: 0.5781 - accuracy: 0.7866 - val_loss: 1.1083 - val_accuracy: 0.6223\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 351s 783ms/step - loss: 0.5596 - accuracy: 0.7965 - val_loss: 1.1240 - val_accuracy: 0.6240\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 346s 772ms/step - loss: 0.5479 - accuracy: 0.7995 - val_loss: 1.1448 - val_accuracy: 0.6229\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 348s 776ms/step - loss: 0.5300 - accuracy: 0.8087 - val_loss: 1.1482 - val_accuracy: 0.6243\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 348s 777ms/step - loss: 0.5152 - accuracy: 0.8101 - val_loss: 1.1623 - val_accuracy: 0.6183\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 349s 778ms/step - loss: 0.5117 - accuracy: 0.8116 - val_loss: 1.1437 - val_accuracy: 0.6285\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 349s 778ms/step - loss: 0.4895 - accuracy: 0.8233 - val_loss: 1.1312 - val_accuracy: 0.6282\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 349s 779ms/step - loss: 0.4688 - accuracy: 0.8313 - val_loss: 1.1941 - val_accuracy: 0.6279\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 350s 782ms/step - loss: 0.4635 - accuracy: 0.8322 - val_loss: 1.1777 - val_accuracy: 0.6210\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 351s 782ms/step - loss: 0.4591 - accuracy: 0.8338 - val_loss: 1.2022 - val_accuracy: 0.6263\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 351s 784ms/step - loss: 0.4480 - accuracy: 0.8378 - val_loss: 1.2254 - val_accuracy: 0.6219\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 351s 782ms/step - loss: 0.4348 - accuracy: 0.8443 - val_loss: 1.2582 - val_accuracy: 0.6217\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 348s 778ms/step - loss: 0.4186 - accuracy: 0.8479 - val_loss: 1.2241 - val_accuracy: 0.6194\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 347s 775ms/step - loss: 0.4159 - accuracy: 0.8508 - val_loss: 1.2546 - val_accuracy: 0.6237\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 351s 783ms/step - loss: 0.4184 - accuracy: 0.8504 - val_loss: 1.2304 - val_accuracy: 0.6267\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 350s 780ms/step - loss: 0.4070 - accuracy: 0.8527 - val_loss: 1.2219 - val_accuracy: 0.6230\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 354s 790ms/step - loss: 0.3829 - accuracy: 0.8616 - val_loss: 1.2825 - val_accuracy: 0.6228\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 356s 794ms/step - loss: 0.3876 - accuracy: 0.8610 - val_loss: 1.2428 - val_accuracy: 0.6257\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 358s 798ms/step - loss: 0.3816 - accuracy: 0.8621 - val_loss: 1.2911 - val_accuracy: 0.6249\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 353s 789ms/step - loss: 0.3777 - accuracy: 0.8637 - val_loss: 1.2845 - val_accuracy: 0.6265\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 356s 794ms/step - loss: 0.3718 - accuracy: 0.8664 - val_loss: 1.2624 - val_accuracy: 0.6244\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 356s 795ms/step - loss: 0.3648 - accuracy: 0.8689 - val_loss: 1.3051 - val_accuracy: 0.6203\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 354s 789ms/step - loss: 0.3513 - accuracy: 0.8749 - val_loss: 1.2987 - val_accuracy: 0.6289\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 360s 804ms/step - loss: 0.3513 - accuracy: 0.8740 - val_loss: 1.3472 - val_accuracy: 0.6246\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 351s 784ms/step - loss: 0.3554 - accuracy: 0.8744 - val_loss: 1.3129 - val_accuracy: 0.6230\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 359s 802ms/step - loss: 0.3496 - accuracy: 0.8770 - val_loss: 1.3585 - val_accuracy: 0.6267\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 354s 790ms/step - loss: 0.3390 - accuracy: 0.8807 - val_loss: 1.3121 - val_accuracy: 0.6300\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 355s 793ms/step - loss: 0.3410 - accuracy: 0.8780 - val_loss: 1.3017 - val_accuracy: 0.6203\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 364s 813ms/step - loss: 0.3387 - accuracy: 0.8792 - val_loss: 1.3358 - val_accuracy: 0.6226\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 360s 804ms/step - loss: 0.3308 - accuracy: 0.8823 - val_loss: 1.3211 - val_accuracy: 0.6264\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network/model\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n",
    "\n",
    "# save model structure in jason file\n",
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save trained model weight in .h5 file\n",
    "emotion_model.save_weights('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25c6da-ddf4-4d45-8681-a273fbebbd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
